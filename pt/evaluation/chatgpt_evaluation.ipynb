{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd \n",
    "import os \n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from os import system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generation the  send data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_header = {\n",
    "    'math': '你是一个数学老师，给定一道数学问题，你需要判断模型回答是否正确，分数在0到1之间。模型回答的最终结果一定要和标准答案中的最终结果一致。模型回答中的解题步骤可以和标准答案不同，但一定要是正确的。请按照\"得分:\"这样的形式输出分数。',\n",
    "    'code': '你是一个计算机科学老师，给定一道编程问题，你需要判断模型回答能否解决该问题，分数在0到1之间。标准答案仅作为参考。模型回答中的代码步骤可以和标准答案不同，但一定要是正确的。请按照\"得分:\"这样的形式输出分数。',\n",
    "    'rewrite': '你需要研究评价标准来对模型回答给出分数，满分为1分，最低分为0分。请按照\"得分:\"这样的形式输出分数 。评价标准要求模型回答能够满足问题中提出的指令。',  # no gold\n",
    "    'classification': '你需要通过参考标准答案，来对模型回答给出分数，满分为1分，最低分为0分。请按照\"得分:\"这样的形式输出分数。评价标准要求模型回答和标准答案越接近越好。',\n",
    "    'generation': '你需要研究评价标准来对模型回答给出分数，满分为1分，最低分为0分。请按照\"得分:\"这样的形式输出分数。评价标准要求模型回答语句通顺，符合问题要求，同时是真实且没有恶意的。',  # no gold\n",
    "    'summarization': '你需要通过参考标准答案，来对模型回答给出分数，满分为1分，最低分为0分。请按照\"得分:\"这样的形式输出分数 。评价标准要求模型回答能包含输入文本信息的重点.',\n",
    "    'extract': '你需要通过参考标准答案，来对模型回答给出分数，满分为1分，最低分为0分。请按照\"得分:\"这样的形式输出分数。评价标准要求模型抽取出来的结果来自问题中给出的上下文，并且很好的回答了该问题。',\n",
    "    'open qa': '你需要通过参考标准答案，来对模型回答给出分数，满分为1分，最低分为0分。请按照\"得分:\"这样的形式输出分数。评价标准要求模型回答越接近标准答案分数越高。',\n",
    "    'translation': '假设你是一个语言学家，你需要通过参考标准答案，来对模型回答给出分数，满分为1分，最低分为0分。请按照\"得分:\"这样的形式输出分数 。评价标准要求翻译过后的句子保持原有的意思，并且翻译过后的句子越通顺分数越高。',\n",
    "    'brainstorming': '你需要研究评价标准来对模型回答给出分数，满分为1分，最低分为0分。请按照\"得分:\"这样的形式输出分数 。评价标准要求模型回答的内容对于问题有帮助，同时是真实且没有恶意的。',   # no gold\n",
    "    'closed qa': '你需要通过参考标准答案，来对模型回答给出分数，满分为1分，最低分为0分。请按照\"得分:\"这样的形式输出分数。评价标准要求模型回答和标准答案越接近越好，且模型回答结果来自问题里面提供的信息。'   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inference_output_path = \"data/send_test_bloom_7ep.jsonl\"\n",
    "test_path = 'eval_data/belle_test.test.json'\n",
    "model_name = inference_output_path.split(\"/\")[-1]\n",
    "output_send_path= f\"data/format_eval_{inference_output_path}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Form test file send to chatgpt'''\n",
    "\n",
    "# read test file\n",
    "test_data = {}\n",
    "with open(test_path, 'r') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        test_data[data['id']] = data\n",
    "\n",
    "\n",
    "\n",
    "unique_id = set()\n",
    "with open(inference_output_path, 'r') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        id= str(data['id'])\n",
    "        if data['id'] not in unique_id:\n",
    "                test_data[id]['model_response'] = data['response']\n",
    "                unique_id.add(id)\n",
    "assert len(test_data) == len(unique_id)\n",
    "\n",
    "# combine for chatgpt evaluation \n",
    "send_data = []\n",
    "for id in test_data:\n",
    "    instruction = test_data[id]['instruction']\n",
    "    gold_response = test_data[id]['gold_response']\n",
    "    model_response = test_data[id]['model_response']\n",
    "    cate = test_data[id]['type']\n",
    "    if cate  in  ['rewrite','generation','brainstorming']:\n",
    "        prompt = f'{prompt_header[cate]} 问题: {instruction} 模型回答: {model_response}' \n",
    "        # prompt = '{} 问题: {} 模型回答: {}'.format(prompt_header[cate], instruction, model_response)\n",
    "    else:\n",
    "        prompt = f'{prompt_header[cate]} 问题: {instruction} 标准答案: {gold_response} 模型回答: {model_response}'\n",
    "        # prompt = '{} 问题: {} 标准答案: {} 模型回答: {}'.format(prompt_header[cate], instruction, gold_response, model_response)\n",
    "    send_data.append({'id':id, 'prompt': prompt})\n",
    "\n",
    "    \n",
    "with open(f\"data/send_data_{model_name}.txt\",'w') as f:\n",
    "    for data in send_data:\n",
    "        f.write(json.dumps(data, ensure_ascii=False)+'\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解析分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Read chatgpt returns and calculate score'''\n",
    "def extract_score(text):\n",
    "        pattern = r\"\\d+(?:\\.\\d+)?分\"  # 匹配数字后面是“分”的字符串\n",
    "        result = re.search(pattern, text)\n",
    "        if result:\n",
    "            return result.group().split(\"分\")[0] # 返回提取结果\n",
    "        else:\n",
    "            pattern = r\"得分:\\d+(?:\\.\\d+)?\"  # 匹配数字后面是“分”的字符串\n",
    "            result = re.search(pattern, text)\n",
    "            if result:\n",
    "                return result.group().split(\"得分:\")[1]  # 返回提取结果\n",
    "            else:\n",
    "                pattern = r\"得分: \\d+(?:\\.\\d+)?\"  # 匹配数字后面是“分”的字符串\n",
    "                result = re.search(pattern, text)\n",
    "                if result:\n",
    "                    return result.group().split(\"得分: \")[1]  # 返回提取结果\n",
    "                else:\n",
    "                    pattern = r\"得分：\\d+(?:\\.\\d+)?\"  # 匹配数字后面是“分”的字符串\n",
    "                    result = re.search(pattern, text)\n",
    "                    if result:\n",
    "                        return result.group().split(\"得分：\")[1]  # 返回提取结果\n",
    "                    else:\n",
    "                    \n",
    "                        pattern = r\"得分为\\d+(?:\\.\\d+)?\"  # 匹配数字后面是“分”的字符串\n",
    "                        result = re.search(pattern, text)\n",
    "                        if result:\n",
    "                            return result.group().split(\"得分为\")[1]  # 返回提取结果                  \n",
    "\n",
    "                    return None \n",
    "\n",
    "def gen_score_details_v2(input_path):\n",
    "    lines = []\n",
    "    cate2scores = defaultdict(list)\n",
    "    allscorles = []\n",
    "    parse_errors = []\n",
    "    reflection = {'math': 'others', 'code': 'others', 'translation': 'rewrite'}\n",
    "    with open(f'{input_path}', 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            id = data['data']['convId']\n",
    "            # assert data['input']['convId'] == data['output']['data']['convId']\n",
    "            prompt = data['data']['prompt']\n",
    "            chatgpt_response = data['data']['responseMessage']\n",
    "            score = extract_score(chatgpt_response)\n",
    "            if score == None or (float(score) > 1 or float(score) < 0):\n",
    "                # print(data)\n",
    "                parse_errors.append(id)\n",
    "                continue\n",
    "            score = float(score)\n",
    "            # previous info\n",
    "            model_response = test_data[id]['model_response']\n",
    "            instruction = test_data[id]['instruction']\n",
    "            gold_response = test_data[id]['gold_response']\n",
    "            cate = test_data[id]['type']\n",
    "            cate = reflection.get(cate, cate)\n",
    "            lines.append({'id':id, 'cate': cate, 'prompt': prompt, 'instruction': instruction, 'gold_response': gold_response, 'model_response': model_response, 'score': score, 'chatgpt_response': chatgpt_response})\n",
    "\n",
    "            cate2scores[cate].append(score)\n",
    "            allscorles.append(score)\n",
    "    print (f'parse error: {len(parse_errors)}')\n",
    "    cate2ave_score = {cate: sum(scores)/len(scores) for cate, scores in cate2scores.items()}\n",
    "    overall_ave_score = sum(cate2ave_score.values())/len(cate2ave_score) # macro average\n",
    "    overall_ave_score_wo_others = sum([cate2ave_score[cate] for cate in cate2ave_score if cate != 'others'])/(len(cate2ave_score)-1)  # macro average\n",
    "    # overall_ave_score = sum(allscorles)/len(allscorles) # micro average\n",
    "    cate2ave_score['macro_ave'] = overall_ave_score\n",
    "    cate2ave_score['macro_ave_wo_others'] = overall_ave_score_wo_others\n",
    "    cate2ave_score['model'] = model_name\n",
    "    print (cate2ave_score)\n",
    "    df = pd.DataFrame([cate2ave_score])\n",
    "    column_order = ['model', 'others', 'rewrite', 'classification', 'generation', 'summarization', 'extract', 'open qa', 'brainstorming', 'closed qa', 'macro_ave', 'macro_ave_wo_others']\n",
    "    df = df[column_order]\n",
    "    # df.to_excel(f'{output_path}/score_{model_name}.xlsx', index=False)\n",
    "\n",
    "    df_detail = pd.DataFrame.from_dict(lines, orient='columns')\n",
    "\n",
    "    return df,df_detail\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gen_score_details_v1(input_path):\n",
    "    lines = []\n",
    "    cate2scores = defaultdict(list)\n",
    "    allscorles = []\n",
    "    parse_errors = []\n",
    "    with open(f'{input_path}', 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            id = data['data']['convId']\n",
    "            # assert data['input']['convId'] == data['output']['data']['convId']\n",
    "            prompt = data['data']['prompt']\n",
    "            chatgpt_response = data['data']['responseMessage']\n",
    "            score = extract_score(chatgpt_response)\n",
    "            if score == None or (float(score) > 1 or float(score) < 0):\n",
    "                # print(data)\n",
    "                parse_errors.append(id)\n",
    "                continue\n",
    "            score = float(score)\n",
    "            # previous info\n",
    "            model_response = test_data[id]['model_response']\n",
    "            instruction = test_data[id]['instruction']\n",
    "            gold_response = test_data[id]['gold_response']\n",
    "            cate = test_data[id]['type']\n",
    "\n",
    "            lines.append({'id':id, 'cate': cate, 'prompt': prompt, 'instruction': instruction, 'gold_response': gold_response, 'model_response': model_response, 'score': score, 'chatgpt_response': chatgpt_response})\n",
    "\n",
    "            cate2scores[cate].append(score)\n",
    "            allscorles.append(score)\n",
    "    print (f'parse error: {len(parse_errors)}')\n",
    "    cate2ave_score = {cate: sum(scores)/len(scores) for cate, scores in cate2scores.items()}\n",
    "    overall_ave_score = sum(cate2ave_score.values())/len(cate2ave_score) # macro average\n",
    "    overall_ave_score_wo_others = sum([cate2ave_score[cate] for cate in cate2ave_score if cate != 'code' and cate!='math'])/(len(cate2ave_score)-2)  # macro average\n",
    "    # overall_ave_score = sum(allscorles)/len(allscorles) # micro average\n",
    "    cate2ave_score['macro_ave'] = overall_ave_score\n",
    "    cate2ave_score['macro_ave_no_code_math'] = overall_ave_score_wo_others\n",
    "    cate2ave_score['model'] = model_name\n",
    "    print (cate2ave_score)\n",
    "    df = pd.DataFrame([cate2ave_score])\n",
    "    column_order = ['model', 'math','code', 'rewrite', 'translation','classification', 'generation', 'summarization', 'extract', 'open qa', 'brainstorming', 'closed qa', 'macro_ave', 'macro_ave_no_code_math']\n",
    "    df = df[column_order]\n",
    "    # df.to_excel(f'{output_path}/score_{model_name}.xlsx', index=False)\n",
    "\n",
    "    df_detail = pd.DataFrame.from_dict(lines, orient='columns')\n",
    "\n",
    "    return df,df_detail\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse error: 2\n",
      "{'open qa': 0.5038596491228068, 'brainstorming': 0.9332402234636875, 'others': 0.4672566371681416, 'summarization': 0.6512820512820515, 'classification': 0.7538461538461536, 'rewrite': 0.8061068702290073, 'generation': 0.944387755102041, 'closed qa': 0.49215686274509807, 'extract': 0.4756756756756757, 'macro_ave': 0.6697568754038515, 'macro_ave_wo_others': 0.6950694051833151, 'model': 'send_test_bloom_7ep.jsonl'}\n"
     ]
    }
   ],
   "source": [
    "input_path=\"data/send_data_send_test_bloom_7ep.jsonl.txt.done\"\n",
    "\n",
    "score_df ,score_df_detailed=  gen_score_details_v2(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>others</th>\n",
       "      <th>rewrite</th>\n",
       "      <th>classification</th>\n",
       "      <th>generation</th>\n",
       "      <th>summarization</th>\n",
       "      <th>extract</th>\n",
       "      <th>open qa</th>\n",
       "      <th>brainstorming</th>\n",
       "      <th>closed qa</th>\n",
       "      <th>macro_ave</th>\n",
       "      <th>macro_ave_wo_others</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>send_test_bloom_7ep.jsonl</td>\n",
       "      <td>0.467257</td>\n",
       "      <td>0.806107</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.944388</td>\n",
       "      <td>0.651282</td>\n",
       "      <td>0.475676</td>\n",
       "      <td>0.50386</td>\n",
       "      <td>0.93324</td>\n",
       "      <td>0.492157</td>\n",
       "      <td>0.669757</td>\n",
       "      <td>0.695069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model    others   rewrite  classification  generation  \\\n",
       "0  send_test_bloom_7ep.jsonl  0.467257  0.806107        0.753846    0.944388   \n",
       "\n",
       "   summarization   extract  open qa  brainstorming  closed qa  macro_ave  \\\n",
       "0       0.651282  0.475676  0.50386        0.93324   0.492157   0.669757   \n",
       "\n",
       "   macro_ave_wo_others  \n",
       "0             0.695069  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cate</th>\n",
       "      <th>prompt</th>\n",
       "      <th>instruction</th>\n",
       "      <th>gold_response</th>\n",
       "      <th>model_response</th>\n",
       "      <th>score</th>\n",
       "      <th>chatgpt_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>open qa</td>\n",
       "      <td>你需要通过参考标准答案，来对模型回答给出分数，满分为1分，最低分为0分。请按照\"得分:\"这样...</td>\n",
       "      <td>什么是机器学习？它有哪些应用场景？\\n</td>\n",
       "      <td>机器学习是一种人工智能科学的分支，其目的是使计算机系统能够自动学习和改进，而不需要严格的编程...</td>\n",
       "      <td>机器学习是一种人工智能（AI）分支，旨在使计算机系统能够从数据中自动学习和改进。简而言之，它...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>得分: 0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>open qa</td>\n",
       "      <td>你需要通过参考标准答案，来对模型回答给出分数，满分为1分，最低分为0分。请按照\"得分:\"这样...</td>\n",
       "      <td>解释什么是自然语言处理，列出至少两个自然语言处理应用场景。\\n</td>\n",
       "      <td>自然语言处理（Natural Language Processing，NLP）是一种计算机科...</td>\n",
       "      <td>自然语言处理（Natural Language Processing, NLP）是一种人工智...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>得分: 0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>brainstorming</td>\n",
       "      <td>你需要研究评价标准来对模型回答给出分数，满分为1分，最低分为0分。请按照\"得分:\"这样的形式...</td>\n",
       "      <td>给出一份关于健康饮食的简短建议。\\n</td>\n",
       "      <td>以下是一些关于健康饮食的简短建议：\\n1. 多食用蔬菜和水果：蔬菜和水果富含维生素、矿物质和...</td>\n",
       "      <td>以下是一些简单而实用的健康饮食建议：\\n1. 多吃蔬菜和水果：每天至少摄入5份新鲜蔬菜或水果...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>得分: 1分。该回答清晰、简洁、准确，对问题给出了有效的健康饮食建议，没有恶意或误导性内容。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>open qa</td>\n",
       "      <td>你需要通过参考标准答案，来对模型回答给出分数，满分为1分，最低分为0分。请按照\"得分:\"这样...</td>\n",
       "      <td>回答以下问题的答案。\\n中国有多少个省？</td>\n",
       "      <td>中国目前有34个省级行政区。具体分为23个省、5个自治区、4个直辖市和2个特别行政区。以下是...</td>\n",
       "      <td>截至2021年，中国的行政区划体系共有34个省级行政区。它们分别是：\\n1. 北京市 - 是...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>得分: 0分。模型回答虽然提到了中国有34个省级行政区，但并未给出完整的具体列表，也没有提到...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>open qa</td>\n",
       "      <td>你需要通过参考标准答案，来对模型回答给出分数，满分为1分，最低分为0分。请按照\"得分:\"这样...</td>\n",
       "      <td>回答以下问题：世界上最长的河流是什么？\\n</td>\n",
       "      <td>世界上最长的河流是尼罗河（Nile River），它位于非洲大陆的东北部，涵盖了11个国家，...</td>\n",
       "      <td>世界上最长的河流是尼罗河。\\n尼罗河位于非洲大陆，从布隆迪和卢旺达的尼亚萨湖流经肯尼亚、坦桑...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>得分: 0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id           cate                                             prompt  \\\n",
       "0   3        open qa  你需要通过参考标准答案，来对模型回答给出分数，满分为1分，最低分为0分。请按照\"得分:\"这样...   \n",
       "1  11        open qa  你需要通过参考标准答案，来对模型回答给出分数，满分为1分，最低分为0分。请按照\"得分:\"这样...   \n",
       "2   6  brainstorming  你需要研究评价标准来对模型回答给出分数，满分为1分，最低分为0分。请按照\"得分:\"这样的形式...   \n",
       "3   2        open qa  你需要通过参考标准答案，来对模型回答给出分数，满分为1分，最低分为0分。请按照\"得分:\"这样...   \n",
       "4  18        open qa  你需要通过参考标准答案，来对模型回答给出分数，满分为1分，最低分为0分。请按照\"得分:\"这样...   \n",
       "\n",
       "                       instruction  \\\n",
       "0              什么是机器学习？它有哪些应用场景？\\n   \n",
       "1  解释什么是自然语言处理，列出至少两个自然语言处理应用场景。\\n   \n",
       "2               给出一份关于健康饮食的简短建议。\\n   \n",
       "3             回答以下问题的答案。\\n中国有多少个省？   \n",
       "4            回答以下问题：世界上最长的河流是什么？\\n   \n",
       "\n",
       "                                       gold_response  \\\n",
       "0  机器学习是一种人工智能科学的分支，其目的是使计算机系统能够自动学习和改进，而不需要严格的编程...   \n",
       "1  自然语言处理（Natural Language Processing，NLP）是一种计算机科...   \n",
       "2  以下是一些关于健康饮食的简短建议：\\n1. 多食用蔬菜和水果：蔬菜和水果富含维生素、矿物质和...   \n",
       "3  中国目前有34个省级行政区。具体分为23个省、5个自治区、4个直辖市和2个特别行政区。以下是...   \n",
       "4  世界上最长的河流是尼罗河（Nile River），它位于非洲大陆的东北部，涵盖了11个国家，...   \n",
       "\n",
       "                                      model_response  score  \\\n",
       "0  机器学习是一种人工智能（AI）分支，旨在使计算机系统能够从数据中自动学习和改进。简而言之，它...    0.7   \n",
       "1  自然语言处理（Natural Language Processing, NLP）是一种人工智...    0.8   \n",
       "2  以下是一些简单而实用的健康饮食建议：\\n1. 多吃蔬菜和水果：每天至少摄入5份新鲜蔬菜或水果...    1.0   \n",
       "3  截至2021年，中国的行政区划体系共有34个省级行政区。它们分别是：\\n1. 北京市 - 是...    0.0   \n",
       "4  世界上最长的河流是尼罗河。\\n尼罗河位于非洲大陆，从布隆迪和卢旺达的尼亚萨湖流经肯尼亚、坦桑...    0.8   \n",
       "\n",
       "                                    chatgpt_response  \n",
       "0                                            得分: 0.7  \n",
       "1                                            得分: 0.8  \n",
       "2     得分: 1分。该回答清晰、简洁、准确，对问题给出了有效的健康饮食建议，没有恶意或误导性内容。  \n",
       "3  得分: 0分。模型回答虽然提到了中国有34个省级行政区，但并未给出完整的具体列表，也没有提到...  \n",
       "4                                            得分: 0.8  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df_detailed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20634b102e5ce792587e2ff69e667181c6bf9c98b3c846a57d51ac3559caad44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
